{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMinuRbkrdzrt4EjBPzYlBC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadeemMughal/RAG_and_CAG/blob/main/CAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing RAG and CAG: A Comprehensive Guide with FAISS Integration and Document Summarization\n",
        "\n",
        "## Author\n",
        "**Muhammad Nadeem**\n",
        "\n",
        "## Introduction\n",
        "This notebook provides a detailed implementation of Retrieval-Augmented Generation (RAG) and Context-Aware Generation (CAG), along with their variations:\n",
        "- **RAG with FAISS**\n",
        "- **CAG with Summarization of Documents**\n",
        "- **CAG without Summarization of Documents**\n",
        "\n",
        "You can use this notebook to understand and execute these methods with your own documents to achieve insightful results.\n",
        "\n",
        "## Implemented Techniques\n",
        "### 1. **Retrieval-Augmented Generation (RAG)**\n",
        "RAG combines retrieval-based approaches with generative models to enhance text generation by leveraging external knowledge sources.\n",
        "\n",
        "### 2. **Context-Aware Generation (CAG)**\n",
        "CAG improves generation by considering the broader context of a document or dataset, ensuring more meaningful and relevant outputs.\n",
        "\n",
        "### 3. **RAG with FAISS**\n",
        "FAISS (Facebook AI Similarity Search) is integrated into RAG for efficient document retrieval, enabling fast and scalable similarity searches.\n",
        "\n",
        "### 4. **CAG with Document Summarization**\n",
        "This approach preprocesses documents by summarizing them before using them for context-aware generation, reducing noise and improving efficiency.\n",
        "\n",
        "### 5. **CAG without Document Summarization**\n",
        "In this method, the full document is used for context-aware generation, ensuring that no information is lost during preprocessing.\n",
        "\n",
        "## How to Use This Code\n",
        "```markdown\n",
        "1. Clone or download the notebook.\n",
        "2. Install the required dependencies.\n",
        "3. Replace the sample documents with your own.\n",
        "4. Run the notebook and analyze the results.\n"
      ],
      "metadata": {
        "id": "oSeN8feiXZ0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG vs CAG Implementation"
      ],
      "metadata": {
        "id": "9PEK2KAhRKAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install PyMuPDF langchain_google_genai google-generativeai faiss-cpu sentence-transformers nltk\n"
      ],
      "metadata": {
        "id": "LhGtjkaoWCC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import fitz  # PyMuPDF for extracting text from PDFs\n",
        "import nltk\n",
        "import numpy as np\n",
        "import faiss\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Load Gemini API Key\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Initialize Gemini model\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=GEMINI_API_KEY)\n",
        "\n",
        "\"\"\"### Step 1: Extract Text from PDF\"\"\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    return \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "\n",
        "pdf_text = extract_text_from_pdf(\"/content/2412.18199v1.pdf\")\n",
        "\n",
        "\"\"\"### Step 2: Chunk Text for Context Window\"\"\"\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def chunk_text(text, max_length=4096):\n",
        "    \"\"\"Splits text into manageable chunks for LLM context window.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks, current_chunk = [], \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "text_chunks = chunk_text(pdf_text, max_length=4096)\n",
        "print(f\"Total Chunks: {len(text_chunks)}\")\n",
        "\n",
        "\"\"\"### Step 3: Store Context in FAISS for Retrieval\"\"\"\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = np.array([embedding_model.encode(chunk) for chunk in text_chunks])\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "context_chunks = text_chunks  # Store original text chunks\n",
        "\n",
        "def retrieve_relevant_chunks(query, top_k=3):\n",
        "    \"\"\"Retrieves the most relevant chunks from FAISS based on query.\"\"\"\n",
        "    query_embedding = embedding_model.encode(query).reshape(1, -1)\n",
        "    _, indices = index.search(query_embedding, top_k)\n",
        "    return [context_chunks[i] for i in indices[0]]\n",
        "\n",
        "\"\"\"### Step 4: Query Gemini Model\"\"\"\n",
        "\n",
        "def query_gemini(user_query, context):\n",
        "    \"\"\"Sends user query with retrieved document context to Gemini.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Given the following context, answer the user's question.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    User Query:\n",
        "    {user_query}\n",
        "    \"\"\"\n",
        "    response = model.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "\"\"\"### Step 5: Implement Conversation Memory\"\"\"\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "def chatbot_response(user_query):\n",
        "    \"\"\"Handles user queries, retrieves relevant document sections, and responds.\"\"\"\n",
        "    relevant_chunks = retrieve_relevant_chunks(user_query)\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "    chat_context = \"\\n\".join(conversation_history[-5:])\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    Previous Conversation:\n",
        "    {chat_context}\n",
        "\n",
        "    Context from Document:\n",
        "    {context}\n",
        "\n",
        "    User Query:\n",
        "    {user_query}\n",
        "    \"\"\"\n",
        "\n",
        "    response = query_gemini(user_query, full_prompt)\n",
        "\n",
        "    conversation_history.append(f\"User: {user_query}\")\n",
        "    conversation_history.append(f\"Chatbot: {response}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\"\"\"### Step 6: Run Chatbot Console\"\"\"\n",
        "\n",
        "def chatbot_console():\n",
        "    \"\"\"Runs the chatbot in the console.\"\"\"\n",
        "    print(\"\\n📄 PDF Chatbot (Retrieval-Augmented Generation - RAG) 📄\")\n",
        "    print(\"Type 'exit' to stop the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYou: \")\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"\\nChatbot: Goodbye! 👋\")\n",
        "            break\n",
        "\n",
        "        response = chatbot_response(user_query)\n",
        "        print(\"\\nChatbot:\", response)\n",
        "\n",
        "# Run chatbot\n",
        "chatbot_console()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXf3Om2wRI__",
        "outputId": "84f6201f-6500-49ce-ed15-6ce57e02819b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.0.9)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.3.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.3.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Chunks: 8\n",
            "\n",
            "📄 PDF Chatbot (Retrieval-Augmented Generation - RAG) 📄\n",
            "Type 'exit' to stop the conversation.\n",
            "\n",
            "You: what's knowledge in your db?\n",
            "\n",
            "Chatbot: I can access and process information from the provided document. The document discusses a system for automating medicine name extraction from handwritten prescriptions using deep learning techniques. It details the architecture of the system, which includes a Feature Pyramid Network (FPN) with ResNet-50 for object detection, a Region Proposal Network (RPN) for generating candidate regions, RoI Align for accurate feature extraction, and a TrOCR model for handwritten text recognition. The document also references several other studies related to handwritten text recognition and medicine name extraction.\n",
            "\n",
            "You: can you give me the authors of it?\n",
            "\n",
            "Chatbot: The document doesn't explicitly list the authors of the system being discussed for automating medicine name extraction. However, it references several papers. If you're interested in the authors of the TrOCR model used in the system, they are Minghao Li, T. Lv, J. Chen, L. Cui, Y. Lu, D. Florencio, C. Zhang, Z. Li, and F. Wei.\n",
            "\n",
            "You: Give me authors of Leveraging Deep Learning with Multi-Head  Attention for Accurate Extraction of Medicine from  Handwritten Prescriptions\n",
            "\n",
            "Chatbot: The authors of \"Leveraging Deep Learning with Multi-Head Attention for Accurate Extraction of Medicine from Handwritten Prescriptions\" are:\n",
            "\n",
            "*   Usman Ali\n",
            "*   Sahil Ranmbail\n",
            "*   Muhammad Nadeem\n",
            "*   Hamid Ishfaq\n",
            "*   Muhammad Umer Ramzan\n",
            "*   Waqas Ali\n",
            "\n",
            "You: also give me emails of it.\n",
            "\n",
            "Chatbot: I am sorry, I cannot provide the emails of the authors as this information is not available in the document.\n",
            "\n",
            "You: give me emails of authors Leveraging Deep Learning with Multi-Head  Attention for Accurate Extraction of Medicine from  Handwritten Prescriptions\n",
            "\n",
            "Chatbot: *   Usman Ali: usmanali@gift.edu.pk\n",
            "*   Sahil Ranmbail: 201980059@gift.edu.pk\n",
            "*   Muhammad Nadeem: 201980050@gift.edu.pk\n",
            "*   Hamid Ishfaq: 201980038@gift.edu.pk\n",
            "*   Muhammad Umer Ramzan: umer.ramzan@gift.edu.pk\n",
            "*   Waqas Ali: waqas.ali@uet.edu.pk\n",
            "\n",
            "You: exit\n",
            "\n",
            "Chatbot: Goodbye! 👋\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJSNxmQSRIlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CAG with Summarization of given document"
      ],
      "metadata": {
        "id": "imsOmF7lO2Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Load Gemini API Key\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Initialize LLM\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=GEMINI_API_KEY)\n",
        "\n",
        "\"\"\"### Step 1: Extract Full Document Text\"\"\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Extract full text from PDF\n",
        "pdf_text = extract_text_from_pdf(\"/content/2412.18199v1.pdf\")\n",
        "\n",
        "\"\"\"### Step 2: Summarize Long Documents (Optional)\"\"\"\n",
        "\n",
        "def summarize_text(text, max_length=8192):\n",
        "    \"\"\"\n",
        "    If the document is too large for the model's context window,\n",
        "    summarize it using the LLM before using it as context.\n",
        "    \"\"\"\n",
        "    if len(text) < max_length:\n",
        "        return text  # Use full text if within model limits\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    The following text is from a research paper. Summarize it in detail while preserving key points:\n",
        "\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    response = model.invoke(prompt)\n",
        "    return response.content  # Return summarized text\n",
        "\n",
        "# Summarize if needed\n",
        "document_context = summarize_text(pdf_text)\n",
        "\n",
        "\"\"\"### Step 3: Modify Query Function to Use Full Context\"\"\"\n",
        "\n",
        "conversation_history = []  # Stores previous conversation\n",
        "\n",
        "def chatbot_response(user_query):\n",
        "    \"\"\"\n",
        "    Uses the entire document as context instead of retrieving chunks.\n",
        "    \"\"\"\n",
        "    # Use last 5 messages for context memory\n",
        "    chat_context = \"\\n\".join(conversation_history[-5:])\n",
        "\n",
        "    # Construct final query\n",
        "    full_prompt = f\"\"\"\n",
        "    Previous Conversation:\n",
        "    {chat_context}\n",
        "\n",
        "    Document Context:\n",
        "    {document_context}  # Entire document or its summary\n",
        "\n",
        "    User Query:\n",
        "    {user_query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Get response from Gemini\n",
        "    response = model.invoke(full_prompt)\n",
        "\n",
        "    # Store conversation history\n",
        "    conversation_history.append(f\"User: {user_query}\")\n",
        "    conversation_history.append(f\"Chatbot: {response}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\"\"\"### Step 4: Run Chatbot Console\"\"\"\n",
        "\n",
        "def chatbot_console():\n",
        "    print(\"\\n📄 PDF Chatbot (Context-Augmented Generation - CAG) 📄\")\n",
        "    print(\"Type 'exit' to stop the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYou: \")\n",
        "\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"\\nChatbot: Goodbye! 👋\")\n",
        "            break\n",
        "\n",
        "        response = chatbot_response(user_query)\n",
        "        print(\"\\nChatbot:\", response)\n",
        "\n",
        "# Run chatbot\n",
        "chatbot_console()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU9nWMwLO12p",
        "outputId": "93069afa-732c-4670-a649-a136c5bae02e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.0.9)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.26.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.3.6)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.3.1)\n",
            "\n",
            "📄 PDF Chatbot (Context-Augmented Generation - CAG) 📄\n",
            "Type 'exit' to stop the conversation.\n",
            "\n",
            "You: what knowledge you have?\n",
            "\n",
            "Chatbot: content='I have knowledge about a research paper that addresses the challenge of extracting medicine names from handwritten doctor prescriptions. I understand the problem statement, the proposed solution (a hybrid approach combining Mask R-CNN and TrOCR), the roles of each model, the dataset used, the string matching technique, the performance achieved, the literature review, the comparison with other models, and the conclusion of the paper. I can answer questions about these aspects based on the provided document context. Essentially, my knowledge is limited to the content of the research paper you provided.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-93ab1405-eca6-472e-94ca-2a15a5708e7d-0' usage_metadata={'input_tokens': 723, 'output_tokens': 109, 'total_tokens': 832, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: Ok, Can you give me the authors of this paper?\n",
            "\n",
            "Chatbot: content=\"content='The authors are from GIFT University and the University of Engineering & Technology Lahore.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-b6a279e3-8f07-499a-8e03-80b13a17e4f8-0' usage_metadata={'input_tokens': 833, 'output_tokens': 18, 'total_tokens': 851, 'input_token_details': {'cache_read': 0}}\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-5a2320fc-d263-4cd3-939a-99a280a82837-0' usage_metadata={'input_tokens': 980, 'output_tokens': 150, 'total_tokens': 1130, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: what author names and what's the conclusion of it?\n",
            "\n",
            "Chatbot: content='content=\"content=\\'The authors are from GIFT University and the University of Engineering & Technology Lahore. The conclusion is that the proposed method is a reliable and efficient tool for automating medicine name extraction from handwritten prescriptions, offering a solution to a long-standing problem in healthcare.\\' additional_kwargs={} response_metadata={\\'prompt_feedback\\': {\\'block_reason\\': 0, \\'safety_ratings\\': []}, \\'finish_reason\\': \\'STOP\\', \\'safety_ratings\\': []} id=\\'run-5982d384-921d-404a-ac2f-17c9946681c7-0\\' usage_metadata={\\'input_tokens\\': 1131, \\'output_tokens\\': 52, \\'total_tokens\\': 1183, \\'input_token_details\\': {\\'cache_read\\': 0}}\" additional_kwargs={} response_metadata={\\'prompt_feedback\\': {\\'block_reason\\': 0, \\'safety_ratings\\': []}, \\'finish_reason\\': \\'STOP\\', \\'safety_ratings\\': []} id=\\'run-1422886c-7a42-48b9-937e-d37bb616b84e-0\\' usage_metadata={\\'input_tokens\\': 1087, \\'output_tokens\\': 185, \\'total_tokens\\': 1272, \\'input_token_details\\': {\\'cache_read\\': 0}}' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-72fc5764-e1c2-4613-9c2f-e5e0e9821ee0-0' usage_metadata={'input_tokens': 1282, 'output_tokens': 321, 'total_tokens': 1603, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: give me author names.\n",
            "\n",
            "Chatbot: content=\"The authors are from GIFT University and the University of Engineering & Technology Lahore. The document doesn't explicitly list the individual author names.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-537441da-08b5-4d36-b119-bf8090b329aa-0' usage_metadata={'input_tokens': 1776, 'output_tokens': 28, 'total_tokens': 1804, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: give me abstract.\n",
            "\n",
            "Chatbot: content='I am sorry, I cannot provide an abstract because the provided document does not contain one. However, I can provide you with a summary.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-8d0f9fe6-e923-479b-9b1b-e3148e09725d-0' usage_metadata={'input_tokens': 1689, 'output_tokens': 29, 'total_tokens': 1718, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: yeah provide me with a summary.\n",
            "\n",
            "Chatbot: content='The research paper presents a novel approach for extracting medicine names from handwritten doctor prescriptions using deep learning. The method combines Mask R-CNN for segmenting medicine name regions in prescription images and TrOCR (Transformer-based Optical Character Recognition) for converting these regions into machine-readable text. A custom dataset of 1,000 handwritten prescriptions, augmented to 9,920 images, was used. The transcribed text is then matched against a database of medicine names using a hybrid string matching technique. The proposed method achieved a character error rate of 1.4% on standard benchmarks. The authors conclude that the system offers a reliable and efficient tool for automating medicine name extraction, addressing a significant challenge in healthcare.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-a39900ff-aefe-49ac-8bcd-dae62f861351-0' usage_metadata={'input_tokens': 1564, 'output_tokens': 145, 'total_tokens': 1709, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: give me literature review.\n",
            "\n",
            "Chatbot: content='The paper reviews existing literature on text recognition, highlighting the limitations of CNN and RNN-based approaches when dealing with handwritten text, blurry images, and the need for extensive pre- and post-processing. It also discusses the advancements in OCR technology and the challenges of recognizing medical information from handwritten prescriptions.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-e0cb6025-26da-4df0-af82-d0a5e2acb927-0' usage_metadata={'input_tokens': 1347, 'output_tokens': 60, 'total_tokens': 1407, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: exit\n",
            "\n",
            "Chatbot: Goodbye! 👋\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CAG without Summarization"
      ],
      "metadata": {
        "id": "MYpo_-edPn29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Load Gemini API Key\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Initialize LLM\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=GEMINI_API_KEY)\n",
        "\n",
        "\"\"\"### Step 1: Extract Full Document Text\"\"\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts the full text from the PDF without summarization.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Extract full text from PDF (No Summarization)\n",
        "document_context = extract_text_from_pdf(\"/content/2412.18199v1.pdf\")\n",
        "\n",
        "\"\"\"### Step 2: Modify Query Function to Use Full Context\"\"\"\n",
        "\n",
        "conversation_history = []  # Stores previous conversation\n",
        "\n",
        "def chatbot_response(user_query):\n",
        "    \"\"\"\n",
        "    Uses the entire document as context instead of retrieving chunks.\n",
        "    \"\"\"\n",
        "    # Use last 5 messages for context memory\n",
        "    chat_context = \"\\n\".join(conversation_history[-5:])\n",
        "\n",
        "    # Construct final query\n",
        "    full_prompt = f\"\"\"\n",
        "    Previous Conversation:\n",
        "    {chat_context}\n",
        "\n",
        "    Document Context:\n",
        "    {document_context}  # Entire document is passed here\n",
        "\n",
        "    User Query:\n",
        "    {user_query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Get response from Gemini\n",
        "    response = model.invoke(full_prompt)\n",
        "\n",
        "    # Store conversation history\n",
        "    conversation_history.append(f\"User: {user_query}\")\n",
        "    conversation_history.append(f\"Chatbot: {response}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\"\"\"### Step 3: Run Chatbot Console\"\"\"\n",
        "\n",
        "def chatbot_console():\n",
        "    print(\"\\n📄 PDF Chatbot (Context-Augmented Generation - CAG) 📄\")\n",
        "    print(\"Type 'exit' to stop the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYou: \")\n",
        "\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"\\nChatbot: Goodbye! 👋\")\n",
        "            break\n",
        "\n",
        "        response = chatbot_response(user_query)\n",
        "        print(\"\\nChatbot:\", response)\n",
        "\n",
        "# Run chatbot\n",
        "chatbot_console()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2G6tvHmMozL",
        "outputId": "102ee2d2-97f3-4d10-8b51-a8855dd87fe1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.0.9)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.26.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.3.6)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.3.1)\n",
            "\n",
            "📄 PDF Chatbot (Context-Augmented Generation - CAG) 📄\n",
            "Type 'exit' to stop the conversation.\n",
            "\n",
            "You: who are the authors of this paper?\n",
            "\n",
            "Chatbot: content='The authors of this paper are:\\n\\n*   Usman Ali\\n*   Sahil Ranmbail\\n*   Muhammad Nadeem\\n*   Hamid Ishfaq\\n*   Muhammad Umer Ramzan\\n*   Waqas Ali' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-e3440842-5607-4055-adfb-7cf2da3b285c-0' usage_metadata={'input_tokens': 7122, 'output_tokens': 49, 'total_tokens': 7171, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: give me all references of this document.\n",
            "\n",
            "Chatbot: content='```\\n[1] S. Gupta, A. Gupta, S. Khanna, and S. Arora, “Digitization of handwritten text using deep learning,” in 2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence), 2022, pp. 596–600.\\n[2] B. Pattanayak, T. Bibhuti, B. Dash, and S. Patra, “A novel technique for handwritten text recognition using easy ocr,” 12 2023.\\n[3] A. Purohit and S. Chauhan, “A literature survey on handwritten character recognition,” International Journal of Computer Science and Information Technology, vol. 7, pp. 1–5, 02 2016.\\n[4] H. Mule, N. Kadam, and D. Naik, “Handwritten text recognition from an image with android application,” in 2022 IEEE Fourth International Conference on Advances in Electronics, Computers and Communications (ICAECC), 2022, pp. 1–5.\\n[5] A. A. Idris and D. B. Taha, “Handwritten text recognition using crnn,” in 2022 8th International Conference on Contemporary Information Technology and Mathematics (ICCITM), 2022, pp. 329–334.\\n[6] T. Jain, R. Sharma, and R. Malhotra, “Handwriting recognition for medical prescriptions using a cnn-bi-lstm model,” 04 2021, pp. 1–4.\\n[7] D. Kulathunga, C. Muthukumarana, U. Pasan, C. Hemachandra, M. Tissera, and H. De Silva, “Patientcare: Patient assistive tool with automatic hand-written prescription reader,” in 2020 2nd International Conference on Advancements in Computing (ICAC), vol. 1, 2020, pp. 275–280.\\n[8] T. Jain, R. Sharma, and R. Malhotra, “Handwriting recognition for medical prescriptions using a cnn-bi-lstm model,” 04 2021, pp. 1–4.\\n[9] R. Ghosh, C. Panda, and P. Kumar, “Handwritten text recognition in bank cheques,” in 2018 Conference on Information and Communication Technology (CICT), 2018, pp. 1–6.\\n[10] A. Ansari, B. Kaur, M. Rakhra, A. Singh, and D. Singh, “Handwritten text recognition using deep learning algorithms,” in 2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST), 2022, pp. 1–6.\\n[11] V. V. Mainkar, J. A. Katkar, A. B. Upade, and P. R. Pednekar, “Handwritten character recognition to obtain editable text,” in 2020 International Conference on Electronics and Sustainable Communication Systems (ICESC).\\nIEEE, 2020, pp. 599–602.\\n[12] C. E. Mook, C. Poo Lee, K. M. Lim, and J. Yan Lim, “Handwritten character and digit recognition with deep convolutional neural networks: A comparative study,” in 2023 11th International Conference on Information and Communication Technology (ICoICT), 2023, pp. 137–141.\\n[13] T. Jain, R. Sharma, and R. Malhotra, “Handwriting recognition for medical prescriptions using a cnn-bi-lstm model,” in 2021 6th International conference for convergence in technology (I2CT). IEEE, 2021, pp. 1–4.\\n[14] L. J. Fajardo, N. J. Sorillo, J. Garlit, C. D. Tomines, M. B. Abisado, J. M. R. Imperial, R. L. Rodriguez, and B. S. Fabito, “Doctor’s cursive handwriting recognition system using deep learning,” in 2019 IEEE 11th international conference on humanoid, nanotechnology, information technology, communication and control, environment, and management (HNICEM).\\nIEEE, 2019, pp. 1–6.\\n[15] U. Shaw, Tania, R. Mamgai, and I. Malhotra, “Medical handwritten prescription recognition and information retrieval using neural network,” in 2021 6th International Conference on Signal Processing, Computing and Control (ISPCC), 2021, pp. 46–50.\\n[16] J. Zia, U. Habib, and M. A. Naeem, “Extraction and classification of medicines from handwritten medical prescriptions,” in 2023 18th International Conference on Emerging Technologies (ICET).\\nIEEE, 2023, pp. 104–109.\\n[17] M. Li, T. Lv, J. Chen, L. Cui, Y. Lu, D. Florencio, C. Zhang, Z. Li, and F. Wei, “Trocr: Transformer-based optical character recognition with pre-trained models,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023, pp. 13 094–13 102.\\n[18] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” 2018.\\n[Online]. Available: https://arxiv.org/abs/1703.06870\\n```' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-3325cca4-ec94-436e-b153-7b71fbbabe9c-0' usage_metadata={'input_tokens': 7324, 'output_tokens': 1302, 'total_tokens': 8626, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: give me methodology\n",
            "\n",
            "Chatbot: content='```\\nThe proposed methodology involves feature extraction using ResNet-50 with a Feature Pyramid Network (FPN), region identification via a Region Proposal Network (RPN), precise alignment with RoI Align, and text isolation through masking. Segmented regions are then processed by the TrOCR model for handwritten text recognition, followed by hybrid string matching for accurate medicine name identification.\\n```' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-404b7bf0-0230-433a-a3de-fae1bda9ad21-0' usage_metadata={'input_tokens': 8770, 'output_tokens': 75, 'total_tokens': 8845, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: give me introduction od it.\n",
            "\n",
            "Chatbot: content=\"content='```\\\\nHandwritten doctor’s prescriptions have long frustrated both patients and pharmacists. The difficulty in deciphering a doctor’s handwriting goes beyond inconvenience; when the handwriting is unclear, it can prevent patients from accurately following medical advice. This problem is compounded by the variability in handwriting styles and prescription formats, making it challenging to develop a standardized text recognition method.\\\\n\\\\nAlthough significant progress has been made, these methods still have certain limitations. Many existing approaches rely on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which, while effective, struggle with the variability and complexity of handwritten text [1], [2], [3]. Techniques such as Bidirectional Long Short-Term Memory (BiLSTM) and Connectionist Temporal Classification (CTC) have also been used, but they often perform poorly on blurry or distorted images [4], [5]. Furthermore, conventional models require extensive pre-processing and post-processing steps, which complicate the systematization of these methods [6], [7], [8].\\\\n\\\\nTo address these limitations, our research makes several significant contributions:\\\\n\\\\n• Proposed a Hybrid Approach: Utilized Mask R-CNN for segmentation and TrOCR for text recognition, followed by the implementation of string matching techniques to accurately retrieve medicine names from the recognized text, comparing results against an existing database.\\\\n\\\\n• Developed an Original Dataset: Created a unique dataset from Pakistan containing various handwritten prescriptions, enabling the model to train on diverse handwriting styles and layouts.\\\\n```' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-e33b6477-e21e-4a5d-a990-0e40283b33e6-0' usage_metadata={'input_tokens': 9076, 'output_tokens': 268, 'total_tokens': 9344, 'input_token_details': {'cache_read': 0}}\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-f1b19d65-b8a5-4301-9f3a-baf23c3da20f-0' usage_metadata={'input_tokens': 8978, 'output_tokens': 447, 'total_tokens': 9425, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: give me abstract of it.\n",
            "\n",
            "Chatbot: content='```\\nExtracting medication names from handwritten doctor prescriptions is challenging due to the wide variability in handwriting styles and prescription formats. This paper presents a robust method for extracting medicine names using a combination of Mask R-CNN and Transformer-based Optical Character Recognition (TrOCR) with Multi-Head Attention and Positional Embeddings. A novel dataset, featuring diverse handwritten prescriptions from various regions of Pakistan, was utilized to fine-tune the model on different handwriting styles. The Mask R-CNN model segments the prescription images to focus on the medicinal sections, while the TrOCR model, enhanced by Multi-Head Attention and Positional Embeddings, transcribes the isolated text. The transcribed text is then matched against a pre-existing database for accurate identification. The proposed approach achieved a character error rate (CER) of 1.4% on standard benchmarks, highlighting its potential as a reliable and efficient tool for automating medicine name extraction.\\n```' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-d4a7c368-d441-4c00-bf58-ff14a92b0dae-0' usage_metadata={'input_tokens': 9369, 'output_tokens': 187, 'total_tokens': 9556, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: ok, my name is Nadeem.\n",
            "\n",
            "Chatbot: content='Okay, Nadeem. How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-3d3f4d80-ad57-4ca6-a5e9-34448362895e-0' usage_metadata={'input_tokens': 8259, 'output_tokens': 13, 'total_tokens': 8272, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: what is my name?\n",
            "\n",
            "Chatbot: content='Based on the document, your name is Muhammad Nadeem.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-35d95e3e-bdbf-43fd-89b5-3cb6c3dc8e4c-0' usage_metadata={'input_tokens': 8198, 'output_tokens': 13, 'total_tokens': 8211, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: my friend name is Ahsan.\n",
            "\n",
            "Chatbot: content=\"Okay, I understand that your friend's name is Ahsan. Is there anything specific you want to know or do with this information, related to the document about extracting medicine names from handwritten prescriptions?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-23cefe0c-0a62-40e0-bfaa-295571c072c0-0' usage_metadata={'input_tokens': 7761, 'output_tokens': 41, 'total_tokens': 7802, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: can you give me my name and my friend name?\n",
            "\n",
            "Chatbot: content='```json\\n{\\n \"response\": \"Okay, based on the document, your name is Muhammad Nadeem and your friend\\'s name is Ahsan.\",\\n \"additional_kwargs\": {},\\n \"response_metadata\": {\\n  \"prompt_feedback\": {\\n   \"block_reason\": 0,\\n   \"safety_ratings\": []\\n  },\\n  \"finish_reason\": \"STOP\",\\n  \"safety_ratings\": []\\n },\\n \"id\": \"run-7974e5c1-4e95-4925-b9e8-9cf754764d54-0\",\\n \"usage_metadata\": {\\n  \"input_tokens\": 8268,\\n  \"output_tokens\": 20,\\n  \"total_tokens\": 8288,\\n  \"input_token_details\": {\\n   \"cache_read\": 0\\n  }\\n }\\n}\\n```' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-a80522bf-3950-426f-9202-28f2e93001d7-0' usage_metadata={'input_tokens': 7615, 'output_tokens': 212, 'total_tokens': 7827, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "You: exit\n",
            "\n",
            "Chatbot: Goodbye! 👋\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pi4NcvYyQF_X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}